---
tags:
  - Note
---
202409222209

Tags : [[Probability Theory]]
# Joint Distribution Measure and Product of Probability
---
If $X_{1},X_{2}\dots X_n$ are random variables, then their joint distribution is the measure $\mu$ on $\mathbb{R}^n$ given by $\omega \mapsto (X_{1}(\omega ), X_{2}(\omega)\dots X_{n}(\omega))$. It is also called the distribution of the vector $X=(X_{1},X_{2}\dots X_{n})$. 

>[!definition] Independence
>Let $\langle \Omega, \mathcal{A}, P\rangle$ be a probability space. Sub-$\sigma$-field $A_{1}\dots A_{n}$ are said to be independent if $P\left( \bigcap A_{i} \right)=\prod P(A_{i})$.
>
>A sequence of sub-fields $\{ A_{i} \}_{i\in\mathbb{N}}$ is called independent if $A_{1}\dots A_{i}$ are independent for all $i\in \mathbb{N}$.

Using this definition we say that random variables $X_{1}$ and $X_{2}$ are independent iff $\sigma(X_{1})$ and $\sigma(X_{2})$ are independent (i.e the sigma field generated by taking inverse images of borel sets are independent).





---
# References
